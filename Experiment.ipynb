{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from operator import itemgetter\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from update_utilities import update_utilities_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Examining the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lord-of-the-rings-processed.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the book - 3729059 characters\n"
     ]
    }
   ],
   "source": [
    "print(f\"length of the book - {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Music of the Ainur There was Eru, the One, who in Arda is called lluvatar; and he made first the\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '®', '—', '‘', '’', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '\"', \"'\", '®', '—', '‘', '’', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "common = r\"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ();:.!?-,\"\n",
    "special = [char for char in chars if char not in list(common)]\n",
    "print(special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace(\"\\n\",\" \")\n",
    "text = text.replace(\"  \", \" \")\n",
    "text = text.replace(\"®\", \"u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‘', '’', '“', '”', ',', ';', ':', '!', '?']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_char = list(itemgetter(*[6,7,8,9])(special))\n",
    "special_char.extend([\",\",\";\",\":\",\"!\",\"?\"])\n",
    "special_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‘', '“']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_space_after = list(itemgetter(*[0,2])(special_char))\n",
    "no_space_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['’', '”', ',', ';', ':', '!', '?']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_space_before = list(itemgetter(*set(range(len(special_char)))-set([0,2]))(special_char))\n",
    "no_space_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace such as <' sss> to <'sss>\n",
    "for s in no_space_after:\n",
    "    text = text.replace(s+\" \", s)\n",
    "\n",
    "# replace such as <s ,> to <s,>\n",
    "for s in no_space_before:\n",
    "    text = text.replace(\" \"+s,s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the use of quotation marks\n",
    "text = text.replace('\"',\"'\")\n",
    "text = text.replace('‘',\"'\")\n",
    "text = text.replace('’',\"'\")\n",
    "text = text.replace('“',\"'\")\n",
    "text = text.replace('”',\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lord-of-the-rings-processed.txt\",\"w\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Create Dictionary and Tokenize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', \"'\", '—']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "common = r\"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ();:.!?-,\"\n",
    "special = [char for char in chars if char not in list(common)]\n",
    "print(special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "encode_char = {char:i for i, char in enumerate(chars)}\n",
    "decode_char = {i:char for i, char in enumerate(chars)}\n",
    "print(len(encode_char))\n",
    "vocab_size = len(encode_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda string: [encode_char[s] for s in string]\n",
    "decode = lambda nums: ''.join([decode_char[n] for n in nums])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 54, 55, 65, 0, 55, 65, 0, 53, 61, 61, 50]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"This is good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0?wXG'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode([8,20,69,44,27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Load data and construct batches + dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**take percentage of text from each book as validation data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_utilities_class(file_name=\"general_functions.py\",current_path=os.getcwd()).run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_functions import HelperFunctionsClass\n",
    "h = HelperFunctionsClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "book1_train, book1_val, end_idx = h.train_test_split(text=text,ending=\"and an end was come for the Eldar of story and of song.\",ratio=0.85,starting_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "book2_train, book2_val, end_idx = h.train_test_split(text=text, ending=\"and handed him the tobacco-jar.\",ratio=0.85,starting_idx=end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "book3_train, book3_val, end_idx = h.train_test_split(text=text, ending=\"THE RETURN OF THE KING.\",ratio=0.85,starting_idx=end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "book4_train, book4_val, end_idx = h.train_test_split(text=text, ending=\"was alive but taken by the Enemy.\",ratio=0.85,starting_idx=end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "book5_train, book5_val, end_idx2 = h.train_test_split(text=text, ending=\"I'm back,' he said.\",ratio=0.85,starting_idx=end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = book1_train + book2_train + book3_train + book4_train + book5_train\n",
    "val_data = book1_val + book2_val + book3_val + book4_val + book5_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3729058"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data + val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data2 = torch.tensor(encode(train_data))\n",
    "val_data2 = torch.tensor(encode(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3170090, 558968)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data2), len(val_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dataset and dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_utilities_class(file_name=\"custom_text_dataset.py\",current_path=os.getcwd()).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_text_dataset import slideTokenizedTextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 512\n",
    "\n",
    "train_dataset = slideTokenizedTextDataset(full_txt = train_data2,\n",
    "                                                 block_size = block_size)\n",
    "\n",
    "val_dataset = slideTokenizedTextDataset(full_txt = val_data2,\n",
    "                                               block_size = block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3169578, 558456)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_num_samples = 200000\n",
    "val_sampler = torch.utils.data.RandomSampler(val_dataset,replacement=False,num_samples=val_num_samples)\n",
    "val_dataloader = DataLoader(dataset=val_dataset,batch_size=batch_size,sampler=val_sampler,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We shall all be caught and killed! I thought you said he was not a friend of theirs.' 'So I did. And don't be silly! You had better go to bed, your wits are sleepy.' The hobbit felt quite crushed, and as there seemed nothing else to do he did go to bed; and while the dwarves were still singing songs he dropped asleep, still puzzling his little head about Beorn, till he dreamed a dream of hundreds of black bears dancing slow heavy dances round and round in the moonlight in the courtyard. Then he woke up when\n",
      "e shall all be caught and killed! I thought you said he was not a friend of theirs.' 'So I did. And don't be silly! You had better go to bed, your wits are sleepy.' The hobbit felt quite crushed, and as there seemed nothing else to do he did go to bed; and while the dwarves were still singing songs he dropped asleep, still puzzling his little head about Beorn, till he dreamed a dream of hundreds of black bears dancing slow heavy dances round and round in the moonlight in the courtyard. Then he woke up when \n"
     ]
    }
   ],
   "source": [
    "s_input, s_output = next(iter(train_dataloader))\n",
    "print(decode(s_input[2].tolist()))\n",
    "print(decode(s_output[2].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49524, 3125)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_utilities_class(file_name=\"Transformer.py\",current_path=os.getcwd()).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Transformer\n",
    "transformer = Transformer.TransformerClass(vocab_size=vocab_size,emb_dim=512,n_layer=6,num_heads=8,block_size=block_size,\n",
    "                               dropout_rate_attention=0.1,dropout_rate_ff=0.1,dropout_rate_pos_enc=0.1, \n",
    "                               is_decoder = True, ff_multiplier = 4).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.98 M parameters\n"
     ]
    }
   ],
   "source": [
    "print(round(sum(p.numel() for p in transformer.parameters())/1e6,2), 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File copied, now the file is available to import from the destinated path\n"
     ]
    }
   ],
   "source": [
    "# update_utilities_class(file_name=\"loss_functions.py\",current_path=os.getcwd()).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import torch\n",
    "from update_utilities import update_utilities_class\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "class train_test_loop_class:\n",
    "    def __init__(self, model:torch.nn.Module, \n",
    "                 train_loader:torch.utils.data.DataLoader,\n",
    "                 val_loader:torch.utils.data.DataLoader,\n",
    "                 test_loader, epochs, print_every_n_batch,\n",
    "                 device,model_name, optimizer,calculate_accuracy,problem_type,overwrite_message,update_loss_fn=False,\n",
    "                 print_result = True, print_full = True, lr_rate_tuning=False, \n",
    "                 clip_batch=False,clip_batch_size=32, lr_start = -5, lr_end = -2):\n",
    "        \n",
    "        # initialize variables\n",
    "        self.train_loader,self.test_loader, self.val_loader = train_loader,test_loader, val_loader\n",
    "        self.model = model\n",
    "        self.epochs, self.device, self.model_name, self.optimizer = epochs, device, model_name,optimizer\n",
    "        self.calculate_accuracy = calculate_accuracy\n",
    "        self.print_progress = print_every_n_batch\n",
    "        self.problem_type = problem_type\n",
    "        self.overwrite_message = overwrite_message\n",
    "        self.print_result, self.print_full = print_result, print_full\n",
    "        self.lr_rate_tuning = lr_rate_tuning\n",
    "        self.clip_batch = clip_batch\n",
    "        self.clip_batch_size = clip_batch_size\n",
    "        self.lr_start, self.lr_end = lr_start, lr_end\n",
    "        \n",
    "        # create folder to hold model stats\n",
    "        model_stats_folder = f\"{self.model_name} stats\"\n",
    "        if not os.path.exists(model_stats_folder):\n",
    "            os.makedirs(model_stats_folder)\n",
    "        self.model_folder = model_stats_folder\n",
    "        \n",
    "        # initialize or read past losses\n",
    "        try:\n",
    "            past_loss_folder = os.path.join(self.model_folder ,f\"{self.model_name} losses\")\n",
    "            past_train_loss_all = list(np.load(past_loss_folder+r\"/train_loss_all.npy\"))\n",
    "            past_train_loss = list(np.load(past_loss_folder+r\"/train_loss.npy\"))\n",
    "            past_validation_loss = list(np.load(past_loss_folder+r\"/validation_loss.npy\"))\n",
    "        except:\n",
    "            self.losses = {\n",
    "                \"train_loss_all\": [],\n",
    "                \"train_loss\": [],\n",
    "                \"validation_loss\": []\n",
    "            }\n",
    "            \n",
    "            self.accuracy = {\n",
    "                \"train_acc_all\": [],\n",
    "                \"train_acc\":[],\n",
    "                \"validation_acc\": []\n",
    "            }\n",
    "        else:\n",
    "            self.losses = {\n",
    "                \"train_loss_all\": past_train_loss_all,\n",
    "                \"train_loss\": past_train_loss,\n",
    "                \"validation_loss\": past_validation_loss\n",
    "            }\n",
    "            if self.calculate_accuracy:\n",
    "                self.accuracy = {\n",
    "                    \"train_acc_all\": list(np.load(past_loss_folder+r\"/train_acc_all.npy\")),\n",
    "                    \"train_acc\": list(np.load(past_loss_folder+r\"/train_acc.npy\")),\n",
    "                    \"validation_acc\": list(np.load(past_loss_folder+r\"/validation_acc.npy\"))\n",
    "                }\n",
    "        \n",
    "        # update and import loss_functions class\n",
    "        if update_loss_fn:\n",
    "            if (print_result & print_full): print(\"update and import the loss_functions module\\n\")\n",
    "            update_file = update_utilities_class(file_name=\"loss_functions.py\",current_path=os.getcwd())\n",
    "            update_file.run()\n",
    "        \n",
    "        from loss_functions import loss_functions_class\n",
    "        loss_function = loss_functions_class()\n",
    "        try:\n",
    "            loss_fn = loss_function.get_loss_fn(self.problem_type)\n",
    "        except:\n",
    "            print(f\"{self.problem_type} Problem Type is not predefined in the loss_functions_class, need to be added manually\")\n",
    "        self.loss_fn = loss_fn\n",
    "        \n",
    "        if (print_result & print_full): print(\"\\nAll initialized, ready to go!\")\n",
    "    \n",
    "    def lr_tuning(self,dataloader,optimizer,start,end,clip_batch, batch_size):\n",
    "        assert start-end < 0, \"start and end should be negative where start less than end\"\n",
    "        if (self.print_result & self.print_full): print(\"learning rate tuning\\n\")\n",
    "        model = copy.deepcopy(self.model)\n",
    "        model.train()\n",
    "        num_step = -(start-end) * 40 + 1\n",
    "        lre = torch.linspace(start,end,num_step)\n",
    "        lrs = 10**lre\n",
    "        lossi = []\n",
    "        i = 0\n",
    "        for batch_inputs, batch_labels in dataloader:\n",
    "            if clip_batch:\n",
    "                batch_inputs, batch_labels = batch_inputs[:min(batch_size,len(batch_inputs))], batch_labels[:min(batch_size,len(batch_labels))]\n",
    "            # define the learning rate\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = lrs[i]\n",
    "            i += 1\n",
    "            # regular forward pass and backpropogation\n",
    "            batch_inputs, batch_labels = batch_inputs.to(self.device), batch_labels.to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            model_outputs = model(batch_inputs)\n",
    "            if \"Binary\" in self.problem_type:\n",
    "                model_outputs = model_outputs.squeeze()\n",
    "                loss = self.loss_fn(model_outputs,batch_labels.float())\n",
    "            elif len(model_outputs.shape) == 2:\n",
    "                loss = self.loss_fn(model_outputs,batch_labels)\n",
    "            else:\n",
    "                loss = self.loss_fn(torch.flatten(model_outputs,end_dim=1),torch.flatten(batch_labels,end_dim=1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lossi.append(loss.detach().cpu().item())\n",
    "            if i == num_step:\n",
    "                if (self.print_result & self.print_full): print(\"learning rate tuning finished\\n\")\n",
    "                del model\n",
    "                return lossi\n",
    "\n",
    "    \n",
    "    def test(self,mode):\n",
    "        self.model.eval()\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0\n",
    "        if mode == \"validation\":\n",
    "            dataloader = self.val_loader\n",
    "        else:\n",
    "            dataloader = self.test_loader\n",
    "            \n",
    "        with torch.inference_mode():\n",
    "            for batch_inputs, batch_labels in dataloader:\n",
    "                batch_inputs, batch_labels = batch_inputs.to(self.device), batch_labels.to(self.device)\n",
    "                model_outputs = self.model(batch_inputs)\n",
    "                if \"Binary\" in self.problem_type:\n",
    "                    model_outputs = model_outputs.squeeze()\n",
    "                    loss = self.loss_fn(model_outputs,batch_labels.float())\n",
    "                elif len(model_outputs.shape) == 2:\n",
    "                    loss = self.loss_fn(model_outputs,batch_labels)\n",
    "                else:\n",
    "                    loss = self.loss_fn(torch.flatten(model_outputs,end_dim=1),torch.flatten(batch_labels,end_dim=1))\n",
    "                batch_loss += loss\n",
    "                if self.calculate_accuracy:\n",
    "                    if \"Binary\" in self.problem_type:\n",
    "                        pred_labels = torch.round(torch.sigmoid(model_outputs))\n",
    "                    else:\n",
    "                        pred_labels = model_outputs.argmax(dim=1)\n",
    "                    acc = torch.eq(pred_labels, batch_labels).sum().item()/len(batch_labels)\n",
    "                    batch_acc += acc\n",
    "            avg_loss = batch_loss / len(dataloader)\n",
    "            avg_acc = batch_acc / len(dataloader) * 100\n",
    "            if mode != \"validation\":\n",
    "                message_file_path = os.path.join(self.model_folder, f\"{self.model_name} - Training Information.txt\")\n",
    "                f = open(message_file_path,\"a\")\n",
    "                f.write(\"\\n\\nTesting Information\\n\"+\"-\"*80)\n",
    "                m = f\"Average per-Batch Test Loss: {avg_loss:.4f}\"\n",
    "                print(m)\n",
    "                f.write(\"\\n\"+m)\n",
    "                if self.calculate_accuracy:\n",
    "                    m = f\"Average per-Batch Test Accuracy: {avg_acc:.2f}%\"\n",
    "                    print(m)\n",
    "                    f.write(\"\\n\"+m)\n",
    "                f.close()\n",
    "            return avg_loss, avg_acc\n",
    "                    \n",
    "        \n",
    "    def train(self):\n",
    "        try:\n",
    "            lowest_val_loss = np.load(os.path.join(self.model_folder,f\"{self.model_name}_lowest_val_loss.npy\")).item()\n",
    "        except:\n",
    "            lowest_val_loss = 1000000000\n",
    "        \n",
    "        start = time.time()\n",
    "        total_time = 0\n",
    "        # write the output to a file\n",
    "        message_file_path = os.path.join(self.model_folder, f\"{self.model_name} - Training Information.txt\")\n",
    "        if self.overwrite_message:\n",
    "            f = open(message_file_path,\"w\")\n",
    "        else:\n",
    "            f = open(message_file_path,\"a\")\n",
    "        \n",
    "        if self.overwrite_message:\n",
    "            m = f\"Basic Specs\\n----------------------------------------------------\"\n",
    "            if (self.print_result & self.print_full): print(m)\n",
    "            f.write(\"\\n\"+m)\n",
    "            sample_inputs, _ = next(iter(self.train_loader))\n",
    "            m = f\"Input Size: {sample_inputs.shape}\\n\"\n",
    "            if (self.print_result & self.print_full): print(m)\n",
    "            f.write(\"\\n\"+m)\n",
    "            m = \"\\nModel Specs: \\n\"\n",
    "            if (self.print_result & self.print_full): print(m)\n",
    "            f.write(\"\\n\"+m)\n",
    "            if (self.print_result & self.print_full): print(self.model)\n",
    "            print(self.model,file=f)\n",
    "            m = \"\\n\\n\"\n",
    "            if (self.print_result & self.print_full): print(m)\n",
    "            f.write(\"\\n\"+m)\n",
    "        \n",
    "        f.write(\"\\n\\nTraining Information\\n\" + \"-\"*80)\n",
    "\n",
    "        \n",
    "        # initializing\n",
    "        num_steps = self.epochs * len(self.train_loader)\n",
    "        progress_bar = tqdm(range(num_steps))\n",
    "        print_progress_cycle = 0 # this keeps track of the current number of print_progress cycle\n",
    "        total_print_progress_cycle = math.ceil(num_steps/self.print_progress)\n",
    "        \n",
    "\n",
    "        \n",
    "        # print initial message\n",
    "        m = f\"Training Begin\\n----------------------------------------------------\"\n",
    "        if (self.print_result & self.print_full): print(m)\n",
    "        f.write(\"\\n\"+m)\n",
    "        m = f\"There are {self.epochs} epochs, and for each epoch, there are {len(self.train_loader)} batches of training data\"\n",
    "        if (self.print_result & self.print_full): print(m)\n",
    "        f.write(\"\\n\"+m)\n",
    "        m = f\"Total Training Steps: {num_steps}\"\n",
    "        if (self.print_result & self.print_full): print(m)\n",
    "        f.write(\"\\n\"+m)\n",
    "        m = f\"Total Displaying Information: {total_print_progress_cycle}\"\n",
    "        if (self.print_result & self.print_full): print(m)\n",
    "        f.write(\"\\n\"+m)\n",
    "        m = f\"Optimizer name - {self.optimizer.__class__.__name__} learning rate: {self.optimizer.param_groups[-1]['lr']}\"\n",
    "        if (self.print_result & self.print_full): print(m)\n",
    "        f.write(\"\\n\"+m)\n",
    "        m = f\"lowest_val_loss started with {lowest_val_loss}\\n\"\n",
    "        if (self.print_result & self.print_full): print(m)\n",
    "        f.write(\"\\n\"+m)\n",
    "        \n",
    "        # initializing\n",
    "        step = 0\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0\n",
    "        \n",
    "        # create directory for the model weights\n",
    "        folder_name = self.model_name + \" weights\"\n",
    "        folder_path = os.path.join(self.model_folder,folder_name)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "        \n",
    "        if self.lr_rate_tuning:\n",
    "            lossi = self.lr_tuning(self.train_loader,self.optimizer,self.lr_start,self.lr_end,self.clip_batch,self.clip_batch_size)\n",
    "            return lossi\n",
    "\n",
    "        # training loop\n",
    "        for e in range(self.epochs):\n",
    "            for batch_inputs, batch_labels in self.train_loader:\n",
    "                step += 1\n",
    "                batch_inputs, batch_labels = batch_inputs.to(self.device), batch_labels.to(self.device)\n",
    "                self.model.train()\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # forward pass\n",
    "                model_outputs = self.model(batch_inputs)\n",
    "                \n",
    "                # loss calculation, backpropogation and update model parameters\n",
    "                if \"Binary\" in self.problem_type:\n",
    "                    model_outputs = model_outputs.squeeze()\n",
    "                    loss = self.loss_fn(model_outputs,batch_labels.float())\n",
    "                elif len(model_outputs.shape) == 2:\n",
    "                    loss = self.loss_fn(model_outputs,batch_labels)\n",
    "                else:\n",
    "                    loss = self.loss_fn(torch.flatten(model_outputs,end_dim=1),torch.flatten(batch_labels,end_dim=1))\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # append loss\n",
    "                self.losses[\"train_loss_all\"].append(loss.detach().cpu().item())\n",
    "                batch_loss += loss\n",
    "                \n",
    "                # if require accuracy calculation\n",
    "                if self.calculate_accuracy:\n",
    "                    if \"Binary\" in self.problem_type:\n",
    "                        pred_labels = torch.round(torch.sigmoid(model_outputs))\n",
    "                    else:\n",
    "                        pred_labels = model_outputs.argmax(dim=1)\n",
    "                    acc = torch.eq(pred_labels,batch_labels).sum().item()/len(batch_labels)\n",
    "                    self.accuracy[\"train_acc_all\"].append(acc)\n",
    "                    batch_acc += acc\n",
    "                \n",
    "                # validate and print progress if reach the print_progress or at the last step\n",
    "                if (step % self.print_progress == 0) | (step == num_steps):\n",
    "                    print_progress_cycle += 1\n",
    "                    if (step != num_steps) | (num_steps % self.print_progress == 0):\n",
    "                        batch_count = self.print_progress\n",
    "                    else:\n",
    "                        batch_count = num_steps % self.print_progress\n",
    "                    \n",
    "                    # print message\n",
    "                    m = f\"\\n\\nMessage: {print_progress_cycle} - \"\\\n",
    "                          +f\"Progress Summary - {batch_count} batches\\n--------------------------------\"\n",
    "                    if (self.print_result & self.print_full): print(m)\n",
    "                    f.write(\"\\n\"+m)\n",
    "                    m = f\"Epoch: {e+1} / {self.epochs} || Batch: {step} / {num_steps} || \" \\\n",
    "                          + f\"Print Cycle: {print_progress_cycle} / {total_print_progress_cycle}\"\n",
    "                    if (self.print_result & self.print_full): print(m)\n",
    "                    f.write(\"\\n\"+m)\n",
    "                    \n",
    "                    validation_loss, validation_acc = self.test(mode=\"validation\")\n",
    "                    self.losses[\"validation_loss\"].append(validation_loss.detach().cpu().item())\n",
    "                    avg_batch_loss = batch_loss/batch_count\n",
    "                    self.losses[\"train_loss\"].append(avg_batch_loss.detach().cpu().item())\n",
    "                    \n",
    "                    # print message\n",
    "                    m = f\"Average per-Batch Training Loss: {avg_batch_loss:.4f} || \" \\\n",
    "                          + f\"Average per-Batch Validation Loss: {validation_loss:.4f}\"\n",
    "                    if (self.print_result & (not self.print_full)): \n",
    "                        print(f\"Batch: {step} / {num_steps} || \" + m)\n",
    "                    if (self.print_result & self.print_full): print(m)\n",
    "                    f.write(\"\\n\"+m)\n",
    "                    \n",
    "                    batch_loss = 0\n",
    "                    \n",
    "                    # if accuracy need be to calculated\n",
    "                    if self.calculate_accuracy:\n",
    "                        self.accuracy[\"validation_acc\"].append(validation_acc)\n",
    "                        avg_batch_acc = batch_acc/batch_count * 100\n",
    "                        self.accuracy[\"train_acc\"].append(avg_batch_acc)\n",
    "                        \n",
    "                        # print message\n",
    "                        m = f\"Average per-Batch Training Accuracy: {avg_batch_acc:.2f}% || \" \\\n",
    "                              + f\"Average per-Batch Validation Accuracy: {validation_acc:.2f}%\"\n",
    "                        print(m)\n",
    "                        if (self.print_result & (not self.print_full)): \n",
    "                            print()\n",
    "                        f.write(\"\\n\"+m)\n",
    "                        \n",
    "                        batch_acc = 0\n",
    "                    \n",
    "                    # calculate model improvement\n",
    "                    if len(self.losses[\"train_loss\"]) > 1:\n",
    "                        idx = len(self.losses[\"train_loss\"]) - 1\n",
    "                        train_loss_perc_decrease = -(self.losses[\"train_loss\"][idx]-self.losses[\"train_loss\"][idx-1]) \\\n",
    "                                                    / self.losses[\"train_loss\"][idx-1] * 100\n",
    "                        val_loss_perc_decrease = -(self.losses[\"validation_loss\"][idx]-self.losses[\"validation_loss\"][idx-1]) \\\n",
    "                                                 / self.losses[\"validation_loss\"][idx-1] * 100\n",
    "                        \n",
    "                        # print message\n",
    "                        m = \"\\nModel Improvement\\n--------------------------------\"\n",
    "                        if (self.print_result & self.print_full): print(m)\n",
    "                        f.write(\"\\n\"+m)\n",
    "                        m = f\"Average per-Batch Training Loss has decreased by {train_loss_perc_decrease:.2f}%\"\n",
    "                        if (self.print_result & self.print_full): print(m)\n",
    "                        f.write(\"\\n\"+m)\n",
    "                        m = f\"Average per-Batch Validation Loss has decreased by {val_loss_perc_decrease:.2f}%\\n\"\n",
    "                        if (self.print_result & self.print_full): print(m)\n",
    "                        f.write(\"\\n\"+m)\n",
    "                        \n",
    "                        # if validation loss is the lowest, save the model as the best model weights\n",
    "                        if validation_loss.cpu() < lowest_val_loss:\n",
    "                            save_path = folder_path + r\"/\"+self.model_name+\"_best.pth\"\n",
    "                            m = f\"Val Loss decreased from {lowest_val_loss:4f} to {validation_loss.cpu():4f} - Saving the Best Model\\n\"\n",
    "                            if (self.print_result & self.print_full): print(m)\n",
    "                            f.write(\"\\n\"+m+\"\\n\")\n",
    "                            torch.save(self.model.state_dict(),save_path)\n",
    "                            lowest_val_loss = validation_loss.cpu()\n",
    "                            np.save(os.path.join(self.model_folder,f\"{self.model_name}_lowest_val_loss.npy\"),lowest_val_loss)\n",
    "                    end = time.time()\n",
    "                    time_spent = np.round((end-start)/60,2)\n",
    "                    total_time += time_spent\n",
    "                    unit = \"minutes\"\n",
    "                    if time_spent > 60:\n",
    "                        time_spent = np.round(time_spent/60,2)\n",
    "                        unit = \"hours\"\n",
    "                    m = f\"This printing cycle took {time_spent} {unit}\\n\"\n",
    "                    if (self.print_result & self.print_full): print(m)\n",
    "                    f.write(\"\\n\"+m)\n",
    "                    start = time.time()\n",
    "                    \n",
    "                \n",
    "                # outside validation and printing\n",
    "                \n",
    "                # update progress bar\n",
    "                progress_bar.update(1)\n",
    "            \n",
    "            # outside dataloader iteration\n",
    "            \n",
    "        # outside epoch for loop\n",
    "        save_path = folder_path + r\"/\"+self.model_name+\"_last.pth\"\n",
    "        m = \"Saving the Last Model\\n\"\n",
    "        if (self.print_result & self.print_full): print(m)\n",
    "        f.write(\"\\n\"+m)\n",
    "        torch.save(self.model.state_dict(),save_path)\n",
    "        \n",
    "        \n",
    "        # save losses/accuracies\n",
    "        loss_folder_name = self.model_name + \" losses\"\n",
    "        loss_folder_path = os.path.join(self.model_folder,loss_folder_name)\n",
    "        if not os.path.exists(loss_folder_path):\n",
    "            os.makedirs(loss_folder_path)\n",
    "        for key in self.losses:\n",
    "            np.save(os.path.join(loss_folder_path,key+\".npy\"),self.losses[key])\n",
    "        if self.calculate_accuracy:\n",
    "            for key in self.accuracy:\n",
    "                np.save(os.path.join(loss_folder_path,key+\".npy\"),self.accuracy[key])\n",
    "        \n",
    "        print(\"\\n All Done\\n\")\n",
    "        time_spent = np.round(total_time/60,2)\n",
    "        m = f\"Overall training took {time_spent} hours\\n\"\n",
    "        print(m)\n",
    "        f.write(\"\\n\\n\"+m+\"-\"*80+\"\\n\\n\\n\\n\")\n",
    "        f.close()\n",
    "\n",
    "    # outside train function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(transformer.parameters(),lr=1e-3)\n",
    "\n",
    "train_loop = train_test_loop_class(model=transformer,train_loader=train_dataloader,val_loader=val_dataloader,test_loader=None, epochs=1, print_every_n_batch=400,\n",
    "                                   device=device,model_name=\"test\",optimizer=optimizer,calculate_accuracy=False,overwrite_message=True, problem_type = \"Multiclass Classification\",\n",
    "                                   update_loss_fn=False, print_result = True, print_full = False, lr_rate_tuning=False,clip_batch=False,clip_batch_size=20,lr_start=-5,lr_end=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffb5168a1c5472a8050325fb78fe1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 400 / 49524 || Average per-Batch Training Loss: 2.4257 || Average per-Batch Validation Loss: 2.2034\n",
      "Batch: 800 / 49524 || Average per-Batch Training Loss: 1.7675 || Average per-Batch Validation Loss: 1.3902\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loop\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn[37], line 301\u001b[0m, in \u001b[0;36mtrain_test_loop_class.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_result \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_full): \u001b[38;5;28mprint\u001b[39m(m)\n\u001b[0;32m    299\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mm)\n\u001b[1;32m--> 301\u001b[0m validation_loss, validation_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(validation_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    303\u001b[0m avg_batch_loss \u001b[38;5;241m=\u001b[39m batch_loss\u001b[38;5;241m/\u001b[39mbatch_count\n",
      "Cell \u001b[1;32mIn[37], line 135\u001b[0m, in \u001b[0;36mtrain_test_loop_class.test\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_inputs, batch_labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m--> 135\u001b[0m         batch_inputs, batch_labels \u001b[38;5;241m=\u001b[39m batch_inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), batch_labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    136\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch_inputs)\n\u001b[0;32m    137\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem_type:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loop.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_space",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
